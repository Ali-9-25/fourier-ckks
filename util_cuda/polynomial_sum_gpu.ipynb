{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-19T22:19:37.199899Z",
     "iopub.status.busy": "2025-04-19T22:19:37.199728Z",
     "iopub.status.idle": "2025-04-19T22:19:37.203832Z",
     "shell.execute_reply": "2025-04-19T22:19:37.203143Z",
     "shell.execute_reply.started": "2025-04-19T22:19:37.199883Z"
    },
    "id": "iFtsTcDPOyn9",
    "outputId": "b6150f86-4262-47b4-8153-976343a166bc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup cuda environment\n",
    "# !pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
    "# %load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-20T13:15:59.126633Z",
     "iopub.status.busy": "2025-04-20T13:15:59.126079Z",
     "iopub.status.idle": "2025-04-20T13:15:59.132595Z",
     "shell.execute_reply": "2025-04-20T13:15:59.131986Z",
     "shell.execute_reply.started": "2025-04-20T13:15:59.126606Z"
    },
    "id": "aKekzMmyXeB_",
    "outputId": "5971c836-2351-460b-ce95-c7a548e8879f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing polySumGPU.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile polySumGPU.cu\n",
    "#include <cstdio>\n",
    "#include <iostream>\n",
    "#define CUDA_CHECK(call)                                                     \\\n",
    "  do {                                                                        \\\n",
    "    cudaError_t err = call;                                                   \\\n",
    "    if (err != cudaSuccess) {                                                 \\\n",
    "      fprintf(stderr, \"CUDA error at %s:%d: %s\\n\",                            \\\n",
    "              __FILE__, __LINE__, cudaGetErrorString(err));                   \\\n",
    "      exit(EXIT_FAILURE);                                                     \\\n",
    "    }                                                                         \\\n",
    "  } while (0)\n",
    "\n",
    "// CUDA kernel function\n",
    "__global__ void poly_sum_kernel(int *input1, int *input2, int size) {\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int stride = gridDim.x * blockDim.x;\n",
    "    for (; tid < size; tid += stride) {\n",
    "        input2[tid] = input1[tid] + input2[tid];\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\" int get_max_threads_per_block() {\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, /*device=*/0);\n",
    "    return prop.maxThreadsPerBlock;\n",
    "}\n",
    "\n",
    "// Wrapper function to call the CUDA kernel\n",
    "extern \"C\" void poly_sum(int *input1, int *input2, int size) {\n",
    "    // Allocate device memory\n",
    "    int *d_input1, *d_input2;\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_input1, size * sizeof(int)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_input2, size * sizeof(int)));\n",
    "\n",
    "    // Copy input data to device\n",
    "    CUDA_CHECK(cudaMemcpy(d_input1, input1, size * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    CUDA_CHECK(cudaMemcpy(d_input2, input2, size * sizeof(int), cudaMemcpyHostToDevice));\n",
    "\n",
    "    // Launch CUDA kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
    "\n",
    "    // Query max active blocks per multiprocessor\n",
    "    int device;\n",
    "    CUDA_CHECK(cudaGetDevice(&device));\n",
    "\n",
    "    cudaFuncAttributes attr;\n",
    "    CUDA_CHECK(cudaFuncGetAttributes(&attr, poly_sum_kernel));\n",
    "\n",
    "    cudaDeviceProp prop;\n",
    "    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n",
    "\n",
    "    std::cout << prop.maxGridSize[0];\n",
    "    // In case number of needed blocks exceeds hardware limit\n",
    "    blocksPerGrid = std::min(blocksPerGrid, prop.maxGridSize[0]);\n",
    "    poly_sum_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_input1, d_input2, size);\n",
    "\n",
    "    // Copy result back to host\n",
    "    CUDA_CHECK(cudaMemcpy(input2, d_input2, size * sizeof(int), cudaMemcpyDeviceToHost));\n",
    "\n",
    "    // Free device memory\n",
    "    cudaFree(d_input1);\n",
    "    cudaFree(d_input2);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:16:01.363440Z",
     "iopub.status.busy": "2025-04-20T13:16:01.362792Z",
     "iopub.status.idle": "2025-04-20T13:16:03.744201Z",
     "shell.execute_reply": "2025-04-20T13:16:03.743376Z",
     "shell.execute_reply.started": "2025-04-20T13:16:01.363416Z"
    },
    "id": "KDUg-yWsX3cJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the cuda code and produce a shared library to get linked to the python main program\n",
    "!nvcc  -o polySumGPU.so -shared -Xcompiler -fPIC polySumGPU.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Polynomial Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-20T13:16:13.028023Z",
     "iopub.status.busy": "2025-04-20T13:16:13.027743Z",
     "iopub.status.idle": "2025-04-20T13:16:13.286173Z",
     "shell.execute_reply": "2025-04-20T13:16:13.285410Z",
     "shell.execute_reply.started": "2025-04-20T13:16:13.028002Z"
    },
    "id": "JRbo0I-yX-Mb",
    "outputId": "e8fd9194-f976-4067-d69d-681d73ee77a1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [6, 4, 12, 11]\n"
     ]
    }
   ],
   "source": [
    "# Python function calling the compiled C++/CUDA function\n",
    "\n",
    "# ctypes in python bridges the gap between python dynamic data types and c static ones.\n",
    "import ctypes\n",
    "\n",
    "# Load the CUDA library\n",
    "cuda_lib = ctypes.CDLL('./polySumGPU.so')  # Update with the correct path\n",
    "\n",
    "# Define the function prototype\n",
    "cuda_lib.poly_sum.argtypes = [ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n",
    "cuda_lib.poly_sum.restype = None\n",
    "\n",
    "# Prepare data\n",
    "input_data_1 = [1, 2, 3, 4]\n",
    "input_data_2 = [5, 2, 9, 7]\n",
    "size = len(input_data_1)\n",
    "\n",
    "# Convert Python lists to ctypes arrays\n",
    "input_array_1 = (ctypes.c_int * size)(*input_data_1)\n",
    "input_array_2 = (ctypes.c_int * size)(*input_data_2)\n",
    "\n",
    "# Call the CUDA function\n",
    "cuda_lib.poly_sum(input_array_1, input_array_2, size)\n",
    "\n",
    "# Print the result\n",
    "result = list(input_array_2)\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Large Input Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "# 1) Load your shared library and set up the prototype\n",
    "cuda_lib = ctypes.CDLL('./polySumGPU.so')  # adjust if your .so has a different name\n",
    "cuda_lib.poly_sum.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_int),  # input1\n",
    "    ctypes.POINTER(ctypes.c_int),  # input2 (result is written here)\n",
    "    ctypes.c_int                   # size\n",
    "]\n",
    "cuda_lib.poly_sum.restype = None\n",
    "\n",
    "def test_poly_sum(N: int):\n",
    "    # 2) Build two big “polynomials” of length N\n",
    "    input1 = list(range(N))\n",
    "    input2 = list(range(N, 2*N))\n",
    "    # 3) Reference sum on CPU\n",
    "    expected = [a + b for a, b in zip(input1, input2)]\n",
    "\n",
    "    # 4) Marshall into ctypes arrays\n",
    "    ArrayType = ctypes.c_int * N\n",
    "    c_in1 = ArrayType(*input1)\n",
    "    c_in2 = ArrayType(*input2)\n",
    "\n",
    "    # 5) Call the GPU kernel once for the entire N\n",
    "    cuda_lib.poly_sum(c_in1, c_in2, N)\n",
    "\n",
    "    # 6) Copy back and compare\n",
    "    gpu_out = list(c_in2)\n",
    "    if gpu_out != expected:\n",
    "        # find the first mismatch\n",
    "        for i, (g, e) in enumerate(zip(gpu_out, expected)):\n",
    "            if g != e:\n",
    "                print(f\"Mismatch at index {i}: GPU={g}  CPU={e}\")\n",
    "                break\n",
    "        raise AssertionError(\"GPU result does not match CPU result!\")\n",
    "    print(f\"[PASS] N={N}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # choose N well above your thread‐per‐block (256) or even blocksPerGrid\n",
    "    for N in [256, 1024, 10_000, 100_000]:\n",
    "        test_poly_sum(N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:16:42.104872Z",
     "iopub.status.busy": "2025-04-20T13:16:42.104594Z",
     "iopub.status.idle": "2025-04-20T13:16:42.114857Z",
     "shell.execute_reply": "2025-04-20T13:16:42.114220Z",
     "shell.execute_reply.started": "2025-04-20T13:16:42.104853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input pairs:\n",
      "  [1, 2, 3, 4] + [10, 20, 30, 40]\n",
      "  [5, 6, 7, 8] + [50, 60, 70, 80]\n",
      "  [9, 10, 11, 12] + [90, 100, 110, 120]\n",
      "Resulting sums:\n",
      "  [11, 22, 33, 44]\n",
      "  [55, 66, 77, 88]\n",
      "  [99, 110, 121, 132]\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "from typing import List, Tuple\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load your CUDA library and define its prototype\n",
    "# -----------------------------------------------------------------------------\n",
    "cuda_lib = ctypes.CDLL('./polySumGPU.so')  # or whatever your .so is named\n",
    "cuda_lib.poly_sum.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_int),  # int *input1\n",
    "    ctypes.POINTER(ctypes.c_int),  # int *input2  (and result is written here)\n",
    "    ctypes.c_int                   # int   size\n",
    "]\n",
    "cuda_lib.poly_sum.restype = None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Batch‐sum helper\n",
    "# -----------------------------------------------------------------------------\n",
    "def polynomial_sum_batch(\n",
    "    polynomial_pairs: List[Tuple[List[int], List[int]]],\n",
    "    degree: int\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Given a list of (polyA, polyB) pairs, each of length `degree + 1`,\n",
    "    computes element‐wise sum polyA + polyB on the GPU in one go,\n",
    "    and returns a list of resulting polynomials.\n",
    "    \"\"\"\n",
    "    n_pairs = len(polynomial_pairs)\n",
    "    if n_pairs == 0:\n",
    "        return []\n",
    "\n",
    "    # --- Validate & flatten into two big lists ---\n",
    "    num_elements = degree + 1\n",
    "    flat_a = []\n",
    "    flat_b = []\n",
    "    for idx, (a, b) in enumerate(polynomial_pairs):\n",
    "        if len(a) != num_elements or len(b) != num_elements:\n",
    "            raise ValueError(\n",
    "                f\"Pair #{idx} has lengths ({len(a)}, {len(b)}), \"\n",
    "                f\"but expected both == {num_elements}\"\n",
    "            )\n",
    "        flat_a.extend(a)\n",
    "        flat_b.extend(b)\n",
    "\n",
    "    total_size = len(flat_a)  # == n_pairs * num_elements\n",
    "\n",
    "    # --- Build ctypes arrays ---\n",
    "    ArrayType = ctypes.c_int * total_size\n",
    "    arr_a = ArrayType(*flat_a)\n",
    "    arr_b = ArrayType(*flat_b)\n",
    "\n",
    "    # --- Call the CUDA kernel ---\n",
    "    cuda_lib.poly_sum(arr_a, arr_b, total_size)\n",
    "\n",
    "    # --- Read back and un-flatten ---\n",
    "    result_flat = list(arr_b)  # kernel writes result into the second array\n",
    "\n",
    "    # split into `n_pairs` chunks of length `num_elements`\n",
    "    result = [\n",
    "        result_flat[i*num_elements : (i+1)*num_elements]\n",
    "        for i in range(n_pairs)\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Example usage\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # three pairs of degree-3 polynomials\n",
    "    polys = [\n",
    "        ([1,2,3,4],  [10,20,30,40]),\n",
    "        ([5,6,7,8],  [50,60,70,80]),\n",
    "        ([9,10,11,12], [90,100,110,120]),\n",
    "    ]\n",
    "    deg = 3\n",
    "\n",
    "    summed = polynomial_sum_batch(polys, deg)\n",
    "    print(\"Input pairs:\")\n",
    "    for a,b in polys:\n",
    "        print(\" \", a, \"+\", b)\n",
    "    print(\"Resulting sums:\")\n",
    "    for r in summed:\n",
    "        print(\" \", r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def polynomial_sum_batch(polynomials, degree, coeff_mod):\n",
    "#     # TODO: Polynomilas should be a list of pairs where each pair is 2 polynomials we need to sum\n",
    "#     # TODO: Concat first set of elements of the pair into one least and the second pair into another\n",
    "#     # TODO: Call the kernel using these 2 lists and don't forget to find the length of one of them\n",
    "#     # TODO: Divide the result into 'x' objects where each one has length of degree\n",
    "#     # TODO: return this new list result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89sBrBJ-OzrL"
   },
   "source": [
    "### **Resources**\n",
    "\n",
    "*   https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n",
    "*   https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/#overlapping_kernel_execution_and_data_transfers\n",
    "*   https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/\n",
    "*   https://vitalitylearning.medium.com/using-c-c-and-cuda-functions-as-regular-python-functions-716f01f7ca22\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
